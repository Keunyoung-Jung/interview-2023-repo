# pytorch-model-class
pytorch í”„ë ˆì„ì›Œí¬ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ëœ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ class ì €ì¥ì†Œ ì…ë‹ˆë‹¤.    
íŒŒì´í”„ë¼ì¸, í›ˆë ¨ ë“±ì„ ìœ„í•´ ëª¨ë¸ì˜ í´ë˜ìŠ¤ë¥¼ ë¯¸ë¦¬ ì €ì¥í•´ë‘ê³  ë²„ì ¼ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.    
ì—¬ê¸°ì„œ `ë² ì´ìŠ¤ ëª¨ë¸`ì€ ì´ë¯¸ ì—°êµ¬ëœ ëª¨ë¸ì˜ ì´ë¦„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.    
`ë² ì´ìŠ¤ ëª¨ë¸`ì˜ ì´ë¦„ì€ [SOTA Link](https://paperswithcode.com/area/natural-language-processing)ì—ì„œ ë“±ì¬ëœ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

# Information
í•˜ë‚˜ì˜ ëª¨ë¸ì´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” êµ¬ì„±ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
* ëª¨ë¸ì´ í›ˆë ¨ ê°€ëŠ¥í•œ í™˜ê²½ì˜ `Dockerfile`
* ëª¨ë¸ì˜ êµ¬ì¡°ê°€ ì‘ì„±ëœ `python script`
* ëª¨ë¸ì„ í•™ìŠµ ì‹œí‚¬ ìˆ˜ ìˆëŠ” ì¿ ë²„ë„¤í‹°ìŠ¤ Yaml íŒŒì¼ `pytorchJob`

# Convention
## ë””ë ‰í† ë¦¬ êµ¬ì¡°
ëª¨ë¸ì˜ Classë¥¼ ì‘ì„±í• ë•ŒëŠ” ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¦…ë‹ˆë‹¤.    
     
ğŸ“ `í”„ë¡œì íŠ¸ëª… directory`    
â”œâ”€ğŸ“ƒ `ë² ì´ìŠ¤ëª¨ë¸ì´ë¦„.py`    
â”œâ”€ğŸ“ƒ `ë² ì´ìŠ¤ëª¨ë¸ì´ë¦„2.py`    
â”œâ”€ğŸ“ƒ `main.py`    
â”œâ”€ğŸ³ `Dockerfile`    
â””â”€âš™ï¸ `pytorchJob.yaml`

## ëª¨ë¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
* ëª¨ë¸ ìŠ¤í¬ë¦½íŠ¸ì˜ ì´ë¦„ì€ ì†Œë¬¸ìë¡œ ì‘ì„±í•©ë‹ˆë‹¤. (ex. `rnn.py`)
### ëª¨ë¸ Class ì‘ì„±
* í´ë˜ìŠ¤ì˜ ì´ë¦„ì€ `BaseModel`ìœ¼ë¡œ(ëŒ€ë¬¸ì ì‹œì‘) ì‘ì„±í•©ë‹ˆë‹¤.
* initì— ëª¨ë¸ì´ë¦„ì„ ê°™ì´ ë„£ì–´ ì‹ë³„ì‘ì—… í•„ìš”ì‹œ ì‚¬ìš©í•©ë‹ˆë‹¤.
```python
class BaseModel(nn.Module) :
    def __init__(self, **args, **kwargs):
        self.name = 'RNN'
        (...)
```
     
### ëª¨ë¸ í›ˆë ¨,ê²€ì¦í•¨ìˆ˜ ì‘ì„±
* ëª¨ë¸ í´ë˜ìŠ¤ ì•ˆì—ëŠ” ëª¨ë¸ê³¼ ê´€ë ¨ëœ í•¨ìˆ˜ë¥¼ ëª¨ë‘ í¬í•¨ì‹œí‚µë‹ˆë‹¤.
* `train_model()`, `evaluate_model()`, `predict()` ì„¸ê°€ì§€ëŠ” ë°˜ë“œì‹œ êµ¬í˜„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
```python
class BaseModel(nn.Module) :
    (...)
    def forward(self, x):
        (...)

    def train_model(self, device, optimizer, train_iter, epoch):
        (...)

    def evaluate_model(self, device, val_iter, epoch):
        (...)
        return avg_loss, avg_accuracy

    def predict(self,text):
        (...)
        return output
```

## `main.py` ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
* í›ˆë ¨ Jobì„ ìœ„í•´ì„œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ëŠ” argparseë¥¼ ì‚¬ìš©í•˜ì—¬ ë°›ë„ë¡ ì‘ì„±í•©ë‹ˆë‹¤.
* `main()` í•¨ìˆ˜ë¥¼ `main.py`ì— ì‘ì„±í•©ë‹ˆë‹¤.
* í•´ë‹¹ ìŠ¤í¬ë¦½íŠ¸ëŠ” í›ˆë ¨ ë° ê²€ì¦ì„ ìœ„í•œ ë™ì‘ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.
```python
def main():
    # Training settings
    parser = argparse.ArgumentParser(description="PyTorch MNIST Example")
    parser.add_argument("--batch-size", type=int, default=64, metavar="N",
                        help="input batch size for training (default: 64)")
    parser.add_argument("--test-batch-size", type=int, default=1000, metavar="N",
                        help="input batch size for testing (default: 1000)")
    parser.add_argument("--epochs", type=int, default=10, metavar="N",
                        help="number of epochs to train (default: 10)")
    parser.add_argument("--lr", type=float, default=0.01, metavar="LR",
                        help="learning rate (default: 0.01)")
    parser.add_argument("--momentum", type=float, default=0.5, metavar="M",
                        help="SGD momentum (default: 0.5)")
    parser.add_argument("--no-cuda", action="store_true", default=False,
                        help="disables CUDA training")
    parser.add_argument("--seed", type=int, default=1, metavar="S",
                        help="random seed (default: 1)")
    parser.add_argument("--log-interval", type=int, default=10, metavar="N",
                        help="how many batches to wait before logging training status")
    parser.add_argument("--log-path", type=str, default="",
                        help="Path to save logs. Print to StdOut if log-path is not set")
    parser.add_argument("--save-model", action="store_true", default=False,
                        help="For Saving the current Model")

    if dist.is_available():
        parser.add_argument("--backend", type=str, help="Distributed backend",
                            choices=[dist.Backend.GLOO, dist.Backend.NCCL, dist.Backend.MPI],
                            default=dist.Backend.GLOO)
    args = parser.parse_args()
```